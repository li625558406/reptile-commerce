"""
Newegg å•†å“çˆ¬è™«è„šæœ¬
åŸºäº Playwright çš„å¼‚æ­¥çˆ¬è™«ï¼Œç”¨äºæŠ“å– Newegg åˆ†ç±»é¡µé¢å•†å“æ•°æ®
"""

import asyncio
import json
import random
import re
from datetime import datetime
from typing import List, Dict, Optional
from playwright.async_api import async_playwright, Page, Browser, Locator


# ==================== é…ç½®åŒºåŸŸ ====================
# åœ¨æ­¤å¤„æ·»åŠ è¦æŠ“å–çš„åˆ†ç±»é“¾æ¥
TARGET_URLS = [
    "https://www.newegg.com/p/pl?Submit=StoreIM&Depa=1",
    "https://www.newegg.com/p/pl?Submit=StoreIM&Depa=2",
    "https://www.newegg.com/p/pl?Submit=StoreIM&Depa=3",
    "https://www.newegg.com/p/pl?Submit=StoreIM&Depa=5",
    "https://www.newegg.com/p/pl?Submit=StoreIM&Depa=6",
    "https://www.newegg.com/p/pl?Submit=StoreIM&Depa=8",
    "https://www.newegg.com/p/pl?Submit=StoreIM&Depa=9",
    "https://www.newegg.com/p/pl?Submit=StoreIM&Depa=10",
    "https://www.newegg.com/p/pl?Submit=StoreIM&Depa=13",
    "https://www.newegg.com/p/pl?Submit=StoreIM&Depa=15",
    "https://www.newegg.com/p/pl?Submit=StoreIM&Depa=16",
    # "https://www.newegg.com/Video-Cards/Video-Card-Series/ID-1805",
    # æ·»åŠ æ›´å¤š URL...
]

# æœ€å¤§æŠ“å–é¡µæ•°ï¼ˆé˜²æ­¢æ— é™æŠ“å–ï¼‰
MAX_PAGES = 10

# User-Agent æ¨¡æ‹ŸçœŸå®æµè§ˆå™¨
USER_AGENT = "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/122.0.0.0 Safari/537.36"

# éšæœºç­‰å¾…èŒƒå›´ï¼ˆç§’ï¼‰
MIN_WAIT = 2
MAX_WAIT = 5


# ==================== æ•°æ®è§£æå‡½æ•° ====================

async def extract_price(price_element: Optional[Locator]) -> str:
    """
    æå–ä»·æ ¼ï¼Œå»é™¤é€—å·å’Œ $ ç¬¦å·
    å¦‚æœæ²¡æœ‰ä»·æ ¼åˆ™è¿”å› "0"
    """
    if not price_element:
        return "0"

    try:
        price_text = await price_element.inner_text()
        # ç§»é™¤ $ ç¬¦å·å’Œé€—å·
        price_clean = price_text.replace("$", "").replace(",", "").strip()
        # éªŒè¯æ˜¯å¦ä¸ºæœ‰æ•ˆä»·æ ¼
        if re.match(r'^\d+\.?\d*$', price_clean):
            return price_clean
        return "0"
    except Exception:
        return "0"


async def extract_item_features(item_cell: Locator) -> List[str]:
    """
    æå–å•†å“ç‰¹æ€§åˆ—è¡¨ (ul.item-features ä¸‹çš„æ‰€æœ‰ li æ–‡æœ¬)
    """
    features = []
    try:
        features_ul = item_cell.locator("ul.item-features")
        if await features_ul.count() > 0:
            feature_items = features_ul.locator("li")
            count = await feature_items.count()
            for i in range(count):
                try:
                    feature_text = await feature_items.nth(i).inner_text()
                    if feature_text.strip():
                        features.append(feature_text.strip())
                except Exception:
                    continue
    except Exception:
        pass
    return features


async def parse_product_item(item_cell: Locator) -> Optional[Dict]:
    """
    è§£æå•ä¸ªå•†å“å¡ç‰‡
    è¿”å›å•†å“ä¿¡æ¯å­—å…¸ï¼Œè§£æå¤±è´¥è¿”å› None
    """
    try:
        # æå–å•†å“æ ‡é¢˜
        title_element = item_cell.locator(".item-title").first
        title = await title_element.inner_text() if await title_element.count() > 0 else ""

        # æå–ä»·æ ¼
        price_element = item_cell.locator(".price-current strong").first
        price = await extract_price(price_element if await price_element.count() > 0 else None)

        # æå–å•†å“å›¾ç‰‡
        img_element = item_cell.locator(".item-img img").first
        img_url = ""
        if await img_element.count() > 0:
            img_url = await img_element.get_attribute("src") or ""

        # æå–å•†å“ç‰¹æ€§åˆ—è¡¨
        item_features = await extract_item_features(item_cell)

        # æå–å•†å“è¯¦æƒ…é“¾æ¥
        link_element = item_cell.locator(".item-title").first
        product_link = ""
        if await link_element.count() > 0:
            href = await link_element.get_attribute("href")
            product_link = href if href and href.startswith("http") else f"https://www.newegg.com{href}" if href else ""

        # æ„å»ºå•†å“æ•°æ®
        product_data = {
            "title": title.strip(),
            "price": price,
            "img_url": img_url,
            "item_features": item_features,
            "product_link": product_link,
        }

        return product_data

    except Exception as e:
        print(f"  âš ï¸  è§£æå•ä¸ªå•†å“å¤±è´¥: {e}")
        return None


async def scrape_page(page: Page, url: str, page_num: int) -> List[Dict]:
    """
    æŠ“å–å•é¡µæ•°æ®
    """
    print(f"\nğŸ“„ æ­£åœ¨æŠ“å–ç¬¬ {page_num} é¡µ: {url}")

    products = []

    try:
        # å¯¼èˆªåˆ°ç›®æ ‡é¡µé¢
        await page.goto(url, wait_until="domcontentloaded", timeout=30000)

        # ç­‰å¾…å•†å“åˆ—è¡¨åŠ è½½
        await page.wait_for_selector(".item-cell", timeout=15000)

        # è·å–æ‰€æœ‰å•†å“å¡ç‰‡
        item_cells = page.locator(".item-cell")
        count = await item_cells.count()
        print(f"  ğŸ“¦ æ‰¾åˆ° {count} ä¸ªå•†å“")

        # éå†è§£ææ¯ä¸ªå•†å“
        for i in range(count):
            item_cell = item_cells.nth(i)
            product = await parse_product_item(item_cell)
            if product:
                products.append(product)
                print(f"    âœ“ [{i+1}/{count}] {product['title'][:50]}...")

        print(f"  âœ… ç¬¬ {page_num} é¡µå®Œæˆï¼ŒæˆåŠŸè§£æ {len(products)} ä¸ªå•†å“")

    except Exception as e:
        print(f"  âŒ æŠ“å–ç¬¬ {page_num} é¡µå¤±è´¥: {e}")

    return products


async def scrape_category(browser: Browser, url: str) -> List[Dict]:
    """
    æŠ“å–æ•´ä¸ªåˆ†ç±»ï¼ˆå¤šé¡µï¼‰
    """
    print(f"\n{'='*60}")
    print(f"ğŸš€ å¼€å§‹æŠ“å–åˆ†ç±»: {url}")
    print(f"{'='*60}")

    # åˆ›å»ºæ–°é¡µé¢
    page = await browser.new_page(user_agent=USER_AGENT)

    all_products = []
    current_url = url

    try:
        for page_num in range(1, MAX_PAGES + 1):
            # æŠ“å–å½“å‰é¡µ
            products = await scrape_page(page, current_url, page_num)
            all_products.extend(products)

            # æ£€æŸ¥æ˜¯å¦æœ‰ä¸‹ä¸€é¡µ
            next_button = page.locator("button[title='Next']").or_(
                page.locator(".pagination .next:not(.disabled)")
            ).or_(
                page.locator("a[title='Next']")
            )

            has_next = await next_button.count() > 0
            is_enabled = False

            if has_next:
                try:
                    is_enabled = await next_button.first.is_enabled()
                except Exception:
                    is_enabled = False

            # å¦‚æœæ²¡æœ‰ä¸‹ä¸€é¡µæˆ–å·²è¾¾åˆ°æœ€å¤§é¡µæ•°ï¼Œåœæ­¢ç¿»é¡µ
            if not has_next or not is_enabled or page_num >= MAX_PAGES:
                if page_num >= MAX_PAGES:
                    print(f"\nâ¹ï¸  å·²è¾¾åˆ°æœ€å¤§é¡µæ•°é™åˆ¶ ({MAX_PAGES})")
                else:
                    print(f"\nâœ… å·²åˆ°è¾¾æœ€åä¸€é¡µ")
                break

            # ç‚¹å‡»ä¸‹ä¸€é¡µ
            print(f"\nâ¡ï¸  å‡†å¤‡ç¿»åˆ°ç¬¬ {page_num + 1} é¡µ...")
            await next_button.first.click()

            # éšæœºç­‰å¾…ï¼Œé˜²æ­¢è¢«åçˆ¬
            wait_time = random.uniform(MIN_WAIT, MAX_WAIT)
            print(f"â±ï¸  ç­‰å¾… {wait_time:.1f} ç§’...")
            await asyncio.sleep(wait_time)

            # è·å–æ–°çš„ URL
            current_url = page.url

    except Exception as e:
        print(f"âŒ æŠ“å–åˆ†ç±»å¤±è´¥: {e}")

    finally:
        await page.close()

    print(f"\nğŸ“Š åˆ†ç±»æŠ“å–å®Œæˆï¼Œå…±è·å– {len(all_products)} ä¸ªå•†å“")
    return all_products


async def main():
    """
    ä¸»å‡½æ•°
    """
    print(f"""
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘         Newegg å•†å“çˆ¬è™« - Playwright ç‰ˆ                  â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    """)

    print(f"ğŸ“‹ é…ç½®:")
    print(f"  - ç›®æ ‡ URL æ•°é‡: {len(TARGET_URLS)}")
    print(f"  - æœ€å¤§é¡µæ•°é™åˆ¶: {MAX_PAGES}")
    print(f"  - éšæœºç­‰å¾…: {MIN_WAIT}-{MAX_WAIT} ç§’")

    all_data = []

    async with async_playwright() as p:
        # å¯åŠ¨æµè§ˆå™¨ï¼ˆæ— å¤´æ¨¡å¼ï¼‰
        browser = await p.chromium.launch(headless=True)

        try:
            # éå†æ‰€æœ‰ç›®æ ‡ URL
            for idx, url in enumerate(TARGET_URLS, 1):
                print(f"\n\n{'#'*60}")
                print(f"# å¤„ç†ç¬¬ {idx}/{len(TARGET_URLS)} ä¸ªåˆ†ç±»")
                print(f"{'#'*60}")

                products = await scrape_category(browser, url)
                all_data.extend(products)

        finally:
            await browser.close()

    # ä¿å­˜æ•°æ®ä¸º JSON
    if all_data:
        filename = f"newegg_data.json"

        with open(filename, "w", encoding="utf-8") as f:
            json.dump(all_data, f, ensure_ascii=False, indent=2)

        print(f"\n\n{'='*60}")
        print(f"ğŸ‰ æŠ“å–å®Œæˆï¼")
        print(f"  - æ€»å•†å“æ•°: {len(all_data)}")
        print(f"  - ä¿å­˜æ–‡ä»¶: {filename}")
        print(f"{'='*60}")
    else:
        print("\nâš ï¸  æœªè·å–åˆ°ä»»ä½•æ•°æ®")


if __name__ == "__main__":
    asyncio.run(main())
